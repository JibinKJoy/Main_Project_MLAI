{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74961b9-2fdc-4fac-bdcb-adbdc67bbbb9",
      "metadata": {
        "id": "b74961b9-2fdc-4fac-bdcb-adbdc67bbbb9",
        "outputId": "56b8f4b7-1080-4ded-b833-c10eba8d1676"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-06 20:21:38.977510: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# import necessary packages for hand gesture recognition project using Python OpenCV\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import cv2\n",
        "import vlc\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594ae213-258e-4b28-9b48-836418515a44",
      "metadata": {
        "id": "594ae213-258e-4b28-9b48-836418515a44"
      },
      "outputs": [],
      "source": [
        "def capture_frames(webcam):\n",
        "    # Read each frame from the webcam\n",
        "    _, frame = webcam.read()\n",
        "\n",
        "    # Flip the frame vertically\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    # Show the final output\n",
        "    cv2.imshow(\"Output\", frame)\n",
        "    return frame\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1202246-0ead-4d7c-9d92-021b7c23f8ea",
      "metadata": {
        "id": "d1202246-0ead-4d7c-9d92-021b7c23f8ea"
      },
      "outputs": [],
      "source": [
        "def get_hand_landmarks(hands, frame):\n",
        "    x , y, c = frame.shape\n",
        "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Get hand landmark prediction\n",
        "    result = hands.process(framergb)\n",
        "    \n",
        "    # import pdb; pdb.set_trace()\n",
        "    if not result.multi_hand_landmarks:\n",
        "        # no hands detected\n",
        "        return []\n",
        "    \n",
        "    # post process the result\n",
        "    landmarks = []\n",
        "    for handslms in result.multi_hand_landmarks:\n",
        "        for lm in handslms.landmark:\n",
        "            # print(id, lm)\n",
        "            lmx = int(lm.x * x)\n",
        "            lmy = int(lm.y * y)\n",
        "            landmarks.append([lmx, lmy])\n",
        "    return landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b36148-a065-46a5-bf6e-bda3ad143eff",
      "metadata": {
        "id": "71b36148-a065-46a5-bf6e-bda3ad143eff"
      },
      "outputs": [],
      "source": [
        "def identify_gesture(model, landmarks, classNames):\n",
        "    # Predict gesture\n",
        "    prediction = model.predict([landmarks])\n",
        "    # print(prediction)\n",
        "    classID = np.argmax(prediction)\n",
        "    className = classNames[classID]\n",
        "    return className\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c925124-c69d-40d2-b8bc-c79d5ea970ee",
      "metadata": {
        "id": "5c925124-c69d-40d2-b8bc-c79d5ea970ee"
      },
      "outputs": [],
      "source": [
        "def invoke_music_controls(media_player, class_name):\n",
        "    \n",
        "    print(f\"{class_name} invoked\")\n",
        "    if class_name == \"thumbs up\":\n",
        "        \n",
        "        media_player.play()\n",
        "    \n",
        "    if class_name == \"stop\":\n",
        "        media_player.stop()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf2e4c77-671c-4c43-8bcc-5106359eb543",
      "metadata": {
        "id": "bf2e4c77-671c-4c43-8bcc-5106359eb543"
      },
      "outputs": [],
      "source": [
        "def get_media_player(player_name=\"vlc\"):\n",
        "    player = vlc.MediaPlayer(\"StarWars60.wav\")\n",
        "    return player"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70f2f80-495f-4077-8ecb-2b27d5927748",
      "metadata": {
        "id": "c70f2f80-495f-4077-8ecb-2b27d5927748"
      },
      "outputs": [],
      "source": [
        "def run_loop(media_player, webcam, hands, model, class_names, delay=0.5):\n",
        "    while True:\n",
        "        class_name = None\n",
        "        landmarks = None\n",
        "        # Read each frame from the webcam\n",
        "        frames = capture_frames(webcam)\n",
        "        landmarks = get_hand_landmarks(hands, frames)\n",
        "        if landmarks:\n",
        "            class_name = identify_gesture(model, landmarks, class_names)\n",
        "            \n",
        "        if class_name:\n",
        "            invoke_music_controls(media_player, class_name)\n",
        "\n",
        "        if cv2.waitKey(1) == ord('q'):\n",
        "                break\n",
        "\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f68845-4457-49ab-bcc0-39719bcf04ab",
      "metadata": {
        "id": "22f68845-4457-49ab-bcc0-39719bcf04ab"
      },
      "outputs": [],
      "source": [
        "def monitor_for_hand_gesture():\n",
        "    # initialize mediapipe\n",
        "    mpHands = mp.solutions.hands\n",
        "    hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
        "    mpDraw = mp.solutions.drawing_utils\n",
        "    \n",
        "    # Load the gesture recognizer model\n",
        "    model = load_model('hand-gesture-recognition-code/mp_hand_gesture')\n",
        "\n",
        "    # Load class names\n",
        "    f = open('hand-gesture-recognition-code/gesture.names', 'r')\n",
        "    class_names = f.read().split('\\n')\n",
        "    f.close()\n",
        "    # print(classNames)\n",
        "    \n",
        "    # Initialize the webcam\n",
        "    webcam = cv2.VideoCapture(0)\n",
        "    \n",
        "    # get media player\n",
        "    media_player = get_media_player()\n",
        "\n",
        "    try:\n",
        "        run_loop(media_player, webcam, hands, model, class_names)\n",
        "    except Exception as e:\n",
        "        raise\n",
        "    finally:\n",
        "        # release the webcam and destroy all active windows\n",
        "        webcam.release()\n",
        "        webcam.destroyAllWindows()\n",
        "        #cv2.waitKey(1) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a958e85e-4261-47c6-851f-44429ae233fa",
      "metadata": {
        "id": "a958e85e-4261-47c6-851f-44429ae233fa",
        "outputId": "d3717b34-277c-4c21-cf4a-f09220c18edb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "2023-01-06 20:21:46.541547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 142ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "rock invoked\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "rock invoked\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "rock invoked\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "live long invoked\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "live long invoked\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "fist invoked\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "thumbs up invoked\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "thumbs up invoked\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "thumbs up invoked\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "thumbs up invoked\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "thumbs up invoked\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "thumbs up invoked\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "fist invoked\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "stop invoked\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "live long invoked\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "okay invoked\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "live long invoked\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "live long invoked\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "live long invoked\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "live long invoked\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'cv2.VideoCapture' object has no attribute 'destroyAllWindows'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmonitor_for_hand_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn [8], line 29\u001b[0m, in \u001b[0;36mmonitor_for_hand_gesture\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# release the webcam and destroy all active windows\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     webcam\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mwebcam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroyAllWindows\u001b[49m()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'cv2.VideoCapture' object has no attribute 'destroyAllWindows'"
          ]
        }
      ],
      "source": [
        "monitor_for_hand_gesture()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ea546a-8811-4526-b8b8-dbc2a07220e1",
      "metadata": {
        "id": "37ea546a-8811-4526-b8b8-dbc2a07220e1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}